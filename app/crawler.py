# app/crawler.py
import logging
import feedparser
import redis
from datetime import datetime, timedelta
from elasticsearch import Elasticsearch
from config import ELASTIC_HOST, ELASTIC_ID, ELASTIC_PASSWORD, REDIS_HOST, ARXIV_API_BASE_URL

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

es_client = Elasticsearch(ELASTIC_HOST, basic_auth=(ELASTIC_ID, ELASTIC_PASSWORD))
redis_client = redis.Redis(host=REDIS_HOST, port=6379, decode_responses=True)

def fetch_arxiv_papers(start_date, end_date, category="cs.CL"):
    """
    arXivì—ì„œ íŠ¹ì • ê¸°ê°„ ë™ì•ˆ ë…¼ë¬¸ì„ í¬ë¡¤ë§
    :param start_date: ì‹œì‘ ë‚ ì§œ (datetime)
    :param end_date: ì¢…ë£Œ ë‚ ì§œ (datetime)
    :param category: arXiv ì¹´í…Œê³ ë¦¬ (ê¸°ë³¸ê°’: cs.CL)
    :return: ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    search_query = f"cat:{category}+AND+submittedDate:[{start_date.strftime('%Y%m%d%H%M%S')}+TO+{end_date.strftime('%Y%m%d%H%M%S')}]"
    query = f"{ARXIV_API_BASE_URL}search_query={search_query}&start=0&max_results=1000"
    
    logger.info(f"ğŸ” Fetching papers from {start_date} to {end_date}")
    return feedparser.parse(query).entries

def store_papers_to_elasticsearch(papers):
    """
    í¬ë¡¤ë§í•œ ë…¼ë¬¸ì„ Elasticsearchì— ì €ì¥
    :param papers: ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    count = 0
    for paper in papers:
        doc = {
            "id": paper.id.split("/")[-1],
            "title": paper.title,
            "authors": [author.name for author in paper.authors],
            "abstract": paper.summary.replace("\n", " "),
            "published_date": paper.published,
            "link": paper.link
        }
        es_client.index(index="arxiv_papers", id=doc["id"], document=doc)
        count += 1
    
    logger.info(f"âœ… Stored {count} papers to Elasticsearch.")
    return count

def crawl_and_store():
    """
    ìµœê·¼ 24ì‹œê°„ ë™ì•ˆì˜ ë…¼ë¬¸ì„ í¬ë¡¤ë§ í›„ Elasticsearchì— ì €ì¥
    """
    end_datetime = datetime.utcnow()
    start_datetime = end_datetime - timedelta(hours=24)

    papers = fetch_arxiv_papers(start_datetime, end_datetime)
    if papers:
        store_papers_to_elasticsearch(papers)
    else:
        logger.warning("âš ï¸ No papers found for this period.")

if __name__ == "__main__":
    crawl_and_store()